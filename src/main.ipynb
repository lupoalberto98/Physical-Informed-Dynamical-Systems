{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d81ea2bf",
   "metadata": {},
   "source": [
    "# Machine Learning Dynamical Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install dependencies\n",
    "!pip3.9 install numpy matplotlib scipy\n",
    "!pip3.9 install tqdm plotly\n",
    "!pip3.9 install torch torchvision torchaudio\n",
    "!pip3.9 install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d7fbfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "%matplotlib inline\n",
    "\n",
    "# Basic import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "\n",
    "# Pytorch import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset, Subset\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import models\n",
    "from models import LSTM, VarFFAE, FFNet\n",
    "\n",
    "# Import training and evaluating functions\n",
    "from training import train\n",
    "from pi_training import pi_train\n",
    "\n",
    "# Import functions for physical informed training\n",
    "from pi_functions import L63_field, PILoss\n",
    "\n",
    "# Import progress bars\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from time import sleep\n",
    "\n",
    "# Optuna\n",
    "import optuna\n",
    "import plotly\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_param_importances, plot_contour\n",
    "\n",
    "\n",
    "#Import sys\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf10d741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network initialized\n",
      "Network initialized\n",
      "Network initialized\n",
      "tensor([[[-0.0890, -0.1729, -0.2102]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### Test nets\n",
    "encoded_space_dim = 2\n",
    "layers_sizes = [3, 16,24]\n",
    "drop_p = 0.3\n",
    "params = {\n",
    "    \"encoded_space_dim\" : encoded_space_dim,\n",
    "    \"layers_sizes\" : layers_sizes,\n",
    "    \"act\" : nn.ReLU(True),\n",
    "    \"drop_p\" : drop_p\n",
    "}\n",
    "\n",
    "\n",
    "net_test = VarFFAE(params)\n",
    "\n",
    "seed_input = torch.tensor((1,1,1), dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "out = net_test(seed_input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64deba8d",
   "metadata": {},
   "source": [
    "## Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768053a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 28.0\n",
    "sigma = 10.0\n",
    "beta = 8.0 / 3.0\n",
    "dt = 0.01\n",
    "\n",
    "def f(state, t):\n",
    "    x, y, z = state  # Unpack the state vector\n",
    "    return sigma * (y - x), x * (rho - z) - y, x * y - beta * z  # Derivatives\n",
    "\n",
    "# Train dataset (discard first 100 steps)\n",
    "state0 = [1.0, 1.0, 1.0]\n",
    "train_steps = 8000\n",
    "t_train = np.arange(0.0, (100+train_steps)*dt, dt)\n",
    "train_dataset = odeint(f, state0, t_train)\n",
    "\n",
    "# Validation dataset (different ic, discard first 100 steps)\n",
    "state1 = [2.0, 0.0, 3.0]\n",
    "val_steps = 2000\n",
    "t_val = np.arange(0.0, (100+val_steps)*dt, dt)\n",
    "val_dataset = odeint(f, state1, t_val)\n",
    "\n",
    "fig = plt.figure(figsize = (10,20))\n",
    "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "ax1.plot(train_dataset[100:, 0], train_dataset[100:, 1], train_dataset[100:, 2])\n",
    "ax1.set_title(\"Train dataset\")\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "ax2.plot(val_dataset[100:, 0], val_dataset[100:, 1], val_dataset[100:, 2])\n",
    "ax2.set_title(\"Validation dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert to tensor\n",
    "l_seq = 20\n",
    "train_sequences = int(train_steps/l_seq)\n",
    "val_sequences = int(val_steps/l_seq)\n",
    "\n",
    "train_states = torch.tensor(train_dataset[100:], requires_grad=True,dtype=torch.float)\n",
    "train_states = torch.reshape(train_states, (train_sequences, l_seq, 3))\n",
    "val_states = torch.tensor(val_dataset[100:], requires_grad=False,dtype=torch.float)\n",
    "val_states = torch.reshape(val_states, (val_sequences, l_seq, 3))\n",
    "\n",
    "print(train_states.shape)\n",
    "print(val_states.shape)\n",
    "### Dataloader\n",
    "torch.manual_seed(0)\n",
    "train_dataloader = DataLoader(train_states, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_states, batch_size=16, shuffle=True)\n",
    "print(next(iter(train_dataloader)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3429777",
   "metadata": {},
   "source": [
    "## Purely data driven approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbe2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter optmization\n",
    "#Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "# Define objective function\n",
    "def objective(trial):\n",
    "    \n",
    "    # Define objects to be optimized\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log = True)\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.0, 1.0)\n",
    "    drop_p = trial.suggest_float(\"dropout\", 0.0, 1.0)\n",
    "    hidden_units = trial.suggest_int(\"hidden_units\", 1,40)\n",
    "    layers_num = trial.suggest_int(\"layers_num\", 1,5)\n",
    "    \n",
    "    # Define network\n",
    "    input_size = 3\n",
    "    model = LSTM(input_size, hidden_units, layers_num, drop_p)\n",
    "\n",
    "   \n",
    "    # Define the optimizer\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, momentum = momentum)\n",
    "        \n",
    "    # Define the loss function \n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    max_num_epochs = 100\n",
    "    early_stopping = False\n",
    "    train_loss, val_loss = train(model, device, train_dataloader, val_dataloader, loss_fn, optimizer, max_num_epochs, early_stopping)\n",
    "    \n",
    "    # Metric to be minimized is the last validation loss\n",
    "    return np.mean(val_loss[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials= 20)\n",
    "study.best_params  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ffd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "input_size = 3\n",
    "hidden_units = 10 #study.best_params[\"hidden_units\"]\n",
    "layers_num =  2 #study.best_params[\"layers_num\"]\n",
    "drop_p = 0.3 #study.best_params[\"dropout\"]\n",
    "torch.manual_seed(0)\n",
    "net_dd = LSTM(input_size, hidden_units, layers_num, drop_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc4a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training\n",
    "\n",
    "\"\"\"\n",
    "# Define the optimizer\n",
    "if  study.best_params[\"optimizer\"]==\"Adam\":\n",
    "    optimizer = getattr(optim, study.best_params[\"optimizer\"])(net_dd.parameters(), lr = study.best_params[\"learning_rate\"])\n",
    "else:\n",
    "    optimizer = getattr(optim, study.best_params[\"optimizer\"])(net_dd.parameters(), lr = study.best_params[\"learning_rate\"], momentum = study.best_params[\"momentum\"])\n",
    "\"\"\"\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(net_dd.parameters())\n",
    "# Define the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "#Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "# Move network to the proper device\n",
    "net_dd.to(device)\n",
    "\n",
    "### Training\n",
    "max_num_epochs = 1600\n",
    "early_stopping = True\n",
    "train_loss, val_loss = train(net_dd, device, train_dataloader, val_dataloader, loss_fn, optimizer, max_num_epochs, early_stopping)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaabb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot losses\n",
    "plt.semilogy(train_loss, label=\"Train loss\")\n",
    "plt.semilogy(val_loss, label=\"Validation loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827630f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find initial state of the RNN\n",
    "\n",
    "# Prepare input\n",
    "seed_input = torch.tensor((1,1,1), dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "seed_input = seed_input.to(device)\n",
    "net_dd.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Forward pass\n",
    "    net_out, initial_net_state = net_dd(seed_input)\n",
    "    \n",
    "print(net_out.shape)\n",
    "print(initial_net_state[0].shape)\n",
    "\n",
    "### Generate a trajectory with network\n",
    "\n",
    "net_input = torch.tensor((0,0,0), dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "net_state = initial_net_state\n",
    "\n",
    "net_states = []\n",
    "for i in range(8000):\n",
    "    with torch.no_grad():\n",
    "        net_input = net_input.to(device)\n",
    "    \n",
    "        # Forward pass\n",
    "        net_input, net_state = net_dd(net_input, net_state)\n",
    "        \n",
    "        net_states.append(net_input[-1].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5506e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_states = np.array(net_states)\n",
    "print(net_states)\n",
    "\n",
    "fig = plt.figure(figsize = (10,20))\n",
    "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "ax1.plot(net_states[900:1400, 0], net_states[900:1400, 1], net_states[900:1400, 2])\n",
    "ax1.set_title(\"Generated dynamics\")\n",
    "ax2.plot(train_dataset[100:, 0],train_dataset[100:, 1], train_dataset[100:, 2])\n",
    "ax2.set_title(\"Train dynamics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645605bd",
   "metadata": {},
   "source": [
    "## Physical informed dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2299037",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter optmization\n",
    "#Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "# Define annealing\n",
    "max_num_epochs = 2000\n",
    "initial_value = 5\n",
    "exp_decay = np.exp(-np.log(initial_value) / max_num_epochs * 6) # We compute the exponential decay in such a way the shape of the exploration profile does not depend on the number of iterations\n",
    "annealing = [initial_value * (exp_decay ** i) for i in range(max_num_epochs)]\n",
    "    \n",
    "# Define loss function\n",
    "loss_fn = PILoss(dt,field = L63_field(rho = 28.0, sigma = 10.0, beta = 8.0/3.0), annealing=annealing)\n",
    "\n",
    "# Define objective function\n",
    "def objective(trial):\n",
    "    \n",
    "    # Define objects to be optimized\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log = True)\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.0, 1.0)\n",
    "    hidden_units = trial.suggest_int(\"hidden_units\", 1,40)\n",
    "    layers_num = trial.suggest_int(\"layers_num\", 1,3)\n",
    "    drop_p = trial.suggest_float(\"dropout\", 0.0, 1.0)\n",
    "   \n",
    "    # Define network\n",
    "    input_size = 3\n",
    "    model = LSTM(input_size, hidden_units, layers_num, drop_p)\n",
    "\n",
    "   \n",
    "    # Define the optimizer\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, momentum = momentum)\n",
    "        \n",
    "    # Train\n",
    "    max_num_epochs = 1000\n",
    "    early_stopping = True\n",
    "    train_loss, val_loss = pi_train(model, device, train_dataloader, val_dataloader, loss_fn, optimizer, max_num_epochs, early_stopping)\n",
    "    \n",
    "    # Metric to be minimized is the last validation loss\n",
    "    return np.mean(val_loss[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a68691",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials= 20)\n",
    "study.best_params  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73590a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "input_size = 3\n",
    "hidden_units = 10#study.best_params[\"hidden_units\"]\n",
    "layers_num =  2#study.best_params[\"layers_num\"]\n",
    "drop_p = 0.3#study.best_params[\"dropout\"]\n",
    "net_pi = LSTM(input_size, hidden_units, layers_num, drop_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ca5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training \n",
    "\n",
    "\"\"\"\n",
    "# Define the optimizer\n",
    "if  study.best_params[\"optimizer\"]==\"Adam\":\n",
    "    optimizer = getattr(optim, study.best_params[\"optimizer\"])(net_pi.parameters(), lr = study.best_params[\"learning_rate\"])\n",
    "else:\n",
    "    optimizer = getattr(optim, study.best_params[\"optimizer\"])(net_pi.parameters(), lr = study.best_params[\"learning_rate\"], momentum = study.best_params[\"momentum\"])\n",
    "\"\"\"\n",
    "optimizer = optim.Adam(net_pi.parameters())\n",
    "# Iterate through the dataloader for \"num_epochs\"\n",
    "max_num_epochs = 2000\n",
    "\n",
    "### Define exploration profile\n",
    "initial_value = 5\n",
    "exp_decay = np.exp(-np.log(initial_value) / max_num_epochs * 6) # We compute the exponential decay in such a way the shape of the exploration profile does not depend on the number of iterations\n",
    "annealing = [initial_value * (exp_decay ** i) for i in range(max_num_epochs)]\n",
    "plt.plot(annealing)\n",
    "# Define loss functions\n",
    "loss_fn = PILoss(dt,field = L63_field(rho = 28.0, sigma = 10.0, beta = 8.0/3.0), annealing = annealing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22672991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "# Move network to the proper device\n",
    "net_pi.to(device)\n",
    "\n",
    "### Training\n",
    "early_stopping = False\n",
    "train_loss, val_loss = pi_train(net_pi, device, train_dataloader, val_dataloader, loss_fn, optimizer, max_num_epochs, early_stopping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbb082",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot losses\n",
    "plt.semilogy(train_loss, label=\"Train loss\")\n",
    "plt.semilogy(val_loss, label=\"Validation loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d85841",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find initial state of the RNN\n",
    "\n",
    "# Prepare input\n",
    "seed_input = torch.tensor((1,1,1), dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "seed_input = seed_input.to(device)\n",
    "net_pi.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Forward pass\n",
    "    net_out, initial_net_state = net_pi(seed_input)\n",
    "    \n",
    "print(net_out.shape)\n",
    "print(initial_net_state[0].shape)\n",
    "\n",
    "### Generate a trajectory with network\n",
    "\n",
    "net_input = torch.tensor((1,1,1), dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "net_state = initial_net_state\n",
    "\n",
    "net_states = []\n",
    "for i in range(8000):\n",
    "    with torch.no_grad():\n",
    "        net_input = net_input.to(device)\n",
    "    \n",
    "        # Forward pass\n",
    "        net_input, net_state = net_pi(net_input, net_state)\n",
    "        \n",
    "        net_states.append(net_input[-1].squeeze().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_states = np.array(net_states)\n",
    "print(net_states.shape)\n",
    "\n",
    "fig = plt.figure(figsize = (10,20))\n",
    "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "ax1.plot(net_states[100:, 0], net_states[100:, 1], net_states[100:, 2])\n",
    "ax1.set_title(\"Generated dynamics\")\n",
    "ax2.plot(train_dataset[100:, 0],train_dataset[100:, 1], train_dataset[100:, 2])\n",
    "ax2.set_title(\"Train dynamics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535c58d",
   "metadata": {},
   "source": [
    "## Lyapunov exponents prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed648e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset generation\n",
    "\n",
    "rho = 28.0\n",
    "sigma = 10.0\n",
    "beta = 8.0 / 3.0\n",
    "dt = 0.01\n",
    "\n",
    "eps = 0.01 # Perturbation\n",
    "len_seq = 5 # Length of the pertubed sequence\n",
    "t = np.arange(0.0, dt*len_seq, dt) # Time array\n",
    "\n",
    "# Add pertubation dimension and dynamics dimension\n",
    "print(states_dataset.shape) \n",
    "pd = np.expand_dims(states_dataset, axis=1)\n",
    "pd = np.expand_dims(pd, axis=1)\n",
    "perturbed_dataset = np.concatenate((pd, pd), axis=2)\n",
    "perturbed_dataset = np.concatenate((perturbed_dataset, pd), axis=2)\n",
    "print(perturbed_dataset.shape)\n",
    "\n",
    "\n",
    "# Add perturbation\n",
    "for i in range(3):\n",
    "    perturbed_dataset[:,:,i,i] += eps\n",
    "    \n",
    "print(perturbed_dataset[0,:,0,:])\n",
    "\n",
    "le_dataset = []\n",
    "# Run the dynamics for all perturbations for len_seq steps\n",
    "for state in perturbed_dataset:\n",
    "    ev_dyn0 = np.expand_dims(odeint(f, state[0,0,:], t), axis=1)\n",
    "    ev_dyn1 = np.expand_dims(odeint(f, state[0,1,:], t), axis=1)\n",
    "    ev_dyn2 = np.expand_dims(odeint(f, state[0,2,:], t), axis=1)\n",
    "   \n",
    "    ev_dyn = np.concatenate((ev_dyn0, ev_dyn1), axis=1)\n",
    "    ev_dyn = np.concatenate((ev_dyn, ev_dyn2), axis=1)\n",
    "    le_dataset.append(ev_dyn)\n",
    "\n",
    "# Convert to numpy\n",
    "le_dataset = np.array(le_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29923c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert to tensor\n",
    "l_seq = 20\n",
    "num_sequences = int(4000/l_seq)\n",
    "\n",
    "le_dataset = torch.tensor(le_dataset, requires_grad=True,dtype=torch.float)\n",
    "\n",
    "\n",
    "### Dataloader\n",
    "le_dataloader = DataLoader(le_dataset, batch_size=16, shuffle=True)\n",
    "print(next(iter(le_dataloader)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14133239",
   "metadata": {},
   "source": [
    "### Trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "input_size = 3\n",
    "hidden_units = 10\n",
    "layers_num = 2\n",
    "drop_p = 0.3\n",
    "net_le = LSTM(input_size, hidden_units, layers_num, drop_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e94279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "# Move network to the proper device\n",
    "net_pi.to(device)\n",
    "# Network in training mode (enable stochastic layers, e.g. dropout)\n",
    "net_pi.train()\n",
    "\n",
    "\n",
    "\n",
    "# Create pbar \n",
    "pbar = tqdm(range(num_epochs))\n",
    "\n",
    "for epoch_num in pbar:\n",
    "    epoch_losses = []\n",
    "    \n",
    "    i = 0\n",
    "    for batch_sample in le_dataloader:\n",
    "        \n",
    "        ### Move samples to the proper device\n",
    "        batch_sample = batch_sample.to(device)\n",
    "\n",
    "        ### Prepare network input and labels\n",
    "        net_input  = batch_sample[:, :-1, :]\n",
    "        labels = batch_sample[:, 1:, :]\n",
    "\n",
    "        ### Forward pass\n",
    "        # Clear previous recorded gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        net_out, _ = net_pi(net_input) # we do not need the rnn state at this point, we can ignore the output with \"_\"\n",
    "    \n",
    "        ### Update network\n",
    "        # Evaluate data driven loss\n",
    "        dd_loss = loss_fn(net_out, labels)\n",
    "        # Evaluate physical informed loss\n",
    "        pi_loss = piloss_fn(net_input,net_out)\n",
    "        \n",
    "        loss = beta[i]*dd_loss + pi_loss\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update\n",
    "        optimizer.step()\n",
    "        # Save batch loss\n",
    "        epoch_losses.append(loss.data.cpu().numpy())\n",
    "        \n",
    "        # Update counter\n",
    "        i = i+1\n",
    "        \n",
    "  \n",
    "    # Compute epoch loss\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "    # Set description\n",
    "    pbar.set_description(\"Train loss: %s\" %round(np.mean(epoch_losses),3))\n",
    "    \n",
    "    # Append\n",
    "    log_loss.append(epoch_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3910jvsc74a57bd0b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
